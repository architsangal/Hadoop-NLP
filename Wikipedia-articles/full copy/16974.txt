[ BKP ] [[File:Knapsack.svg|thumb|right|250px|Example of a one-dimensional (constraint) knapsack problem: which boxes should be chosen to maximize the amount of money while still keeping the overall weight under or equal to 15  kg? A multiple constrained problem could consider both the weight and volume of the boxes. (Solution: if any number of each box is available, then three yellow boxes and three grey boxes; if only the shown boxes are available, then all but the green box.)]] The knapsack problem or rucksack problem is a problem in combinatorial optimization : Given a set of items, each with a weight and a value, determine the number of each item to include in a collection so that the total weight is less than or equal to a given limit and the total value is as large as possible. It derives its name from the problem faced by someone who is constrained by a fixed-size knapsack and must fill it with the most valuable items. The problem often arises in resource allocation where there are financial constraints and is studied in fields such as combinatorics, computer science, complexity theory, cryptography and applied mathematics. The knapsack problem has been studied for more than a century, with early works dating as far back as 1897. [ On the partition of numbers Mathews, G. B. Proceedings of the London Mathematical Society 28 486 –490 25 June 1897 http://plms.oxfordjournals.org/content/s1-28/1/486.full.pdf ] It is not known how the name &quot; knapsack problem &quot; originated, though the problem was referred to as such in the early works of mathematician Tobias Dantzig (1884 –1956), { Dantzig, Tobias. Numbers: The Language of Science, 1930. } suggesting that the name could have existed in folklore before a mathematical problem had been fully defined. { Kellerer, Pferschy, and Pisinger 2004, p. 3 } [ Applications ] A 1998 study of the Stony Brook University algorithms repository showed that, out of 75 algorithmic problems, the knapsack problem was the 18th most popular and the 4th most needed after kd-tree s, suffix tree s, and the bin packing problem. [ Who is Interested in Algorithms and Why? Lessons from the Stony Brook Algorithm Repository Skiena, S. S. AGM SIGACT News 30 3 September 1999 65 –74 0163-5700 http://delivery.acm.org/10.1145/340000/333627/p65-skiena.pdf?key1%3D333627%26amp%3Bkey2%3D9434996821%26amp%3Bcoll%3DGUIDE%26amp%3Bdl%3DGUIDE%26amp%3BCFID%3D108583297%26amp%3BCFTOKEN%3D90100478 ] Knapsack problems appear in real-world decision-making processes in a wide variety of fields, such as the finding the least wasteful way to cut raw materials, { Kellerer, Pferschy, and Pisinger 2004, p. 449 } selection of capital investment s and financial portfolio s, { Kellerer, Pferschy, and Pisinger 2004, p. 461 } selection of assets for asset-backed securitization, { Kellerer, Pferschy, and Pisinger 2004, p. 465 } and generating keys for the Merkle–Hellman knapsack cryptosystem. { Kellerer, Pferschy, and Pisinger 2004, p. 472 } One early application of knapsack algorithms was in the construction and scoring of tests in which the test-takers have a choice as to which questions they answer. For small examples it is a fairly simple process to provide the test-takers with such a choice. For example, if an exam contains 12 questions each worth 10 points, the test-taker need only answer 10 questions to achieve a maximum possible score of 100 points. However, on tests with a heterogeneous distribution of point values —i.e. different questions are worth different point values — it is more difficult to provide choices. Feuerman and Weiss proposed a system in which students are given a heterogeneous test with a total of 125 possible points. The students are asked to answer all of the questions to the best of their abilities. Of the possible subsets of problems whose total point values add up to 100, a knapsack algorithm would determine which subset gives each student the highest possible score. [ A Mathematical Programming Model for Test Construction and Scoring Management Science 19 8 April 1973 961–966 Feuerman, Martin; Weiss, Harvey 2629127 ] [ Mathematical Definition ] The most common formulation of the problem is the 0-1 knapsack problem, which restricts the number x i of copies of each kind of itemto zero or one. The picture above can be interpreted in this way. Mathematically the 0-1-knapsack problem can be formulated as: Let there be n items, x_1 to x_n where x_i has a value v_i and weight w_i. The maximum weight that we can carry in the bag is W.It is common to assume that all values and weights are nonnegative. To simplify the representation, we also assume that the items are listed in increasing order of weight. Maximize \qquad \sum_{i=1}^n v_ix_i subject to \qquad \sum_{i=1}^n w_ix_i \leqslant W, \quad \quad x_i \in \{0,1\} Maximize the sum of the values of the items in the knapsack so that the sum of the weights must be less than the knapsack&apos;s capacity. The bounded knapsack problem removes the restriction that there is only one of each item, but restricts the number x_i of copies ofeach kind of item to an integer value c_i. Mathematically the bounded knapsack problem can be formulated as: maximize \qquad \sum_{i=1}^n v_ix_i subject to \qquad \sum_{i=1}^n w_ix_i \leqslant W, \quad \quad x_i \in \{0,1,\ldots,c_i\} The unbounded knapsack problem (UKP) places no upper bound on the number of copies of each kind of item and can be formulated as above except for that the only restriction on x_i is that it is a positive integer. If the example with the colored bricks above is viewed as an unbounded knapsack problem, then the solution is to take three yellow boxes and three grey boxes. [ The Knapsack Problem in Computer Science ] The knapsack problem is interesting from the perspective of computer science for many reasons: The decision problem form of the knapsack problem ( &quot; Can a value of at least V be achieved without exceeding the weight W?) is NP-complete, thus it is expected that no algorithm can be both correct and fast (polynomial-time) on all cases. While the decision problem is NP-complete the optimization problem is NP-hard, its resolution is at least as difficult as the decision problem, and there is no known polynomial algorithm which can tell, given a solution, whether it is optimal (which would mean that there is no solution with a larger, thus solving the decision problem NP-complete). There is a pseudo-polynomial time algorithm using dynamic programming There is a fully polynomial-time approximation scheme, which uses the pseudo-polynomial time algorithm as a subroutine, described below. Many cases that arise in practice, and &quot; random instances &quot; from some distributions, can nonetheless be solved exactly. There is a link between the &quot; decision &quot; and &quot; optimization &quot; problems in that if there exists a polynomial algorithm that solves the &quot; decision &quot; problem, then one can find the maximum value for the optimization problem in polynomial time by applying this algorithm iteratively while increasing the value of k. On the other hand, if an algorithm finds the optimal value of optimization problem in polynomial time, then the decision problem can be solved in polynomial time by comparing the value of the solution output by this algorithm with the value of k. Thus, both versions of the problem are of similar difficulty. One theme in research literature is to identify what the &quot; hard &quot; instances of the knapsack problem look like, { Pisinger, D. 2003. Where are the hard knapsack problems? Technical Report 2003/08, Department of Computer Science, University of Copenhagen, Copenhagen, Denmark. } { L. Caccetta, A. Kulanoot, Computational Aspects of Hard Knapsack Problems,Nonlinear Analysis 47 (2001) 5547–5558. } or viewed another way, to identify what properties of instances in practice might make them more amenable than their worst-case NP-complete behaviour suggests. &lt; refname = &quot; poirriez et all 2009 &quot; &gt; Vincent Poirriez, Nicola Yanev, Rumen Andonov (2009) A Hybrid Algorithm for the Unbounded Knapsack Problem Discrete Optimization http://dx.doi.org/10.1016/j.disopt.2008.09.004 { The goal in finding these &quot; hard &quot; instances is for their use in } { public key cryptography systems, such as the } { Merkle-Hellman knapsack cryptosystem. } [ Solving The Knapsack Problem ] Several algorithms are freely available to solve knapsack problems, based on dynamic programming approach, { Rumen Andonov, Vincent Poirriez, Sanjay Rajopadhye (2000) Unbounded Knapsack Problem : dynamic programming revisited European Journal of Operational Research 123: 2. 168–181 http://dx.doi.org/10.1016/S0377-2217(99)00265-9 } branch and bound approach { S. Martello, P. Toth, Knapsack Problems: Algorithms and Computer Implementation,John Wiley and Sons, 1990 } or hybridizations of both approaches. &lt; refname= &quot; martellopisingertoth99a &quot; &gt; S. Martello, D. Pisinger, P. Toth, Dynamic programming and strong bounds for the 0-1knapsack problem, Manag. Sci., 45:414–424, 1999. &lt; refname= &quot; plateau85 &quot; &gt; G. Plateau, M. Elkihel, A hybrid algorithm for the 0-1 knapsack problem, Methods ofOper. Res., 49:277–293, 1985. { S. Martello, P. Toth, A mixture of dynamic programming and branch-and-bound for the subset-sum problem, Manag. Sci., 30:765–771 } [ Dynamic Programming Algorithm ] Unbounded knapsack problem If all weights ( w_1,\ldots,w_n,W ) arenonnegative integers, the knapsack problem can be solved in pseudo-polynomial time using dynamic programming. The following describes a dynamic programming solution for the unbounded knapsack problem. To simplify things, assume all weights are strictly positive (w i &gt; 0). We wish to maximize total value subject to the constraint that total weight is less than or equal to W. Then for each w ≤ W, define m[w] to be the maximum value that can be attained with total weight less than or equal to w. m[W] then is the solution to the problem. Observe that m[w] has the following properties: m[0]=0\,\! (the sum of zero items, i.e., the summation of the empty set) m[w]= \max_{w_i \le w}(v_i+m[w-w_i] ) where v_i is the value of the i-th kind of item. The following is pseudo code for the dynamic program: input: values v and weights w for items 1 to n; number of distinct items n; knapsack capacity W { for w from 0 to W do } { T[0, w] := 0 } { end for } { for i from 1 to n do } { for j from 0 to W do } { if j &gt; = w[i] then } { T[i, j] := max(T[i-1, j], T[i-1, j-w[i] ] + v[i]) } { else } { T[i, j] := T[i-1, j] } { end if } { end for } { end for } Here the maximum of the empty set is taken to be zero. Tabulating the results from m[0] up through m[W] gives the solution. Since the calculation of each m[w] involves examining n items, and there are W values of m[w] to calculate, the running time of the dynamic programming solution is O(nW). Dividing w_1,\,w_2,\,\ldots,\,w_n,\,W by their greatest common divisor is an obvious way to improve the running time. The O(nW) complexity does not contradict the fact that the knapsack problem is NP-complete, since W, unlike n, is not polynomial in the length of the input to the problem. The length of the W input to the problem is proportional to the number of bits in W, \log W, not to W itself. 0-1 knapsack problem A similar dynamic programming solution for the 0-1 knapsack problem also runs in pseudo-polynomial time. As above, assume w_1,\,w_2,\,\ldots,\,w_n,\,W are strictly positive integers. Define m[i,w] to be the maximum value that can be attained with weight less than or equal to w using items up to i. We can define m[i,w] recursively as follows: m[i,\,w]=m[i-1,\,w] if w_i &gt; w\,\! (the new item is more than the current weight limit) m[i,\,w]=\max(m[i-1,\,w],\,m[i-1,w-w_i]+v_i) if w_i \leqslant w. The solution can then be found by calculating m[n,W]. To do this efficiently we can use a table to store previous computations. This solution will therefore run in O(nW) time and O(nW) space. Additionally, if we use only a 1-dimensional array m[w] to store the current optimal values and pass over this array i+1 times, rewriting from m[W] to m[1] every time, we get the same result for only O(W) space. [ Meet-in-the-Middle Algorithm ] Another algorithm for 0-1 knapsack, discovered in 1974 [ Horowitz Ellis Sahni Sartaj Sartaj Sahni 10.1145/321812.321823 Journal of the Association for Computing Machinery 0354006 277–292 Computing partitions with applications to the knapsack problem 21 1974 ] and sometimes called &quot; meet-in-the-middle &quot; due to parallels to a similarly named algorithm in cryptography, is exponential in the number of different items but may be preferable to the DP algorithm when W is large compared to n. In particular, if the w_i are nonnegative but not integers, we could still use the dynamic programming algorithm by scaling and rounding (i.e. using fixed-point arithmetic ), but if the problem requires d fractional digits of precision to arrive at the correct answer, W will need to be scaled by 10^d, and the DP algorithm will require O(W*10^d) space and O(nW*10^d) time. &apos;&apos; Meet-in-the-middle algorithm { input: } { a set of items with weights and values } { output: } { the greatest combined value of a subset } { partition the set {1...n} into two sets A and B of approximately equal size } { compute the weights and values of all subsets of each set } { for each subset of A } { find the subset of B of greatest value s.t. the combined weight is less than W } { keep track of the greatest combined value seen so far } The algorithm takes O(2^{n/2}) space, and efficient implementations of step 3 (for instance, sorting the subsets of B by weight, discarding subsets of B which weigh more than other subsets of B of greater or equal value, and using binary search to find the best match) result in a runtime of O(n*2^{n/2}). As with the meet in the middle attack in cryptography, this improves on the O(n*2^n) runtime of a naive brute force approach (examining all subsets of {1...n}), at the cost of using exponential rather than constant space. [ Approximation Algorithms ] As for most NP-complete problems, it may be enough to find workable solutions even if they are not optimal. Preferably, however, the approximation comes with a guarantee on the difference between the value of the solution found and the value of the optimal solution. As with many useful but computationally complex algorithms, there has been substantial research on creating and analyzing algorithms that approximate a solution. The knapsack problem, though NP-Hard, is one of a collection of algorithms that can still be approximated to any specified degree. This means that the problem has a Polynomial Time Approximation Scheme. To be exact, the knapsack problem has a Fully Polynomial Time Approximation Scheme (FPTAS) and runs in pseudo-polynomial time. { Vazirani, Vijay. Approximation Algorithms. Springer-Verlag Berlin Heidelberg, 2003. } Fully Polynomial Time Approximation Scheme for the Knapsack Problem The FPTAS for the Knapsack Problem takes advantage of the fact that the reason the problem is not polynomial time algorithms is because the profits associated with the items are not restricted. If one rounds off some of the least significant digits of the profit values then they will be bounded by a polynomial and 1/ε where ε is a bound on the correctness of the solution. This restriction then means that an algorithm can find a solution in polynomial time that is correct within(1-ε) percent of the optimal solution. &apos;&apos; An Algorithm for FPTAS { input: } { ε∈[0,1], } { a list, A, of n items their values, a_i, and weights } { output: S&apos; the FPTAS solution } { K := εP/n where P=v_i where item i is the highest valued item } { for a_i from 1 to n } { profit&apos;( a_i ) = ⌊profit( a_i )/ K⌋ } { return the solution, S&apos;, using the profit&apos; values in the dynamic program outlined above } Theorem: The set, S&apos;, output by the algorithm above satisfies: profit(S&apos;) ≥ (1-ε)* profit(S*) Where S&apos; is the solution produce by the algorithm and S* is the optimal solution. Greedy Approximation Algorithm George Dantzig proposed a greedy approximation algorithm to solve the unbounded knapsack problem. { George B. Dantzig, Discrete-Variable Extremum Problems, Operations ResearchVol. 5, No. 2, April 1957, pp. 266–288, DOI: http://dx.doi.org/10.1287/opre.5.2.266 } His version sorts the items in decreasing order of value per unit of weight, v_i/w_i. It then proceeds to insert them into the sack, starting with as many copies as possible of the first kind of item until there is no longer space in the sack for more. Provided that there is an unlimited supply of each kind of item, if m is the maximum value of items that fit into the sack, then the greedy algorithm is guaranteed to achieve at least a value of m/2. However, for the bounded problem, where the supply of each kind of item is limited, the algorithm may be far from optimal. [ Dominance relations in the Unbounded Knapsack Problem ] Solving the unbounded knapsack problem can be made easier by throwing away items which will never be needed. For a given item i, suppose we could find a set of items J such that their total weight is less than the weight of i, and their total value is greater than the value of i. Then i cannot appear in the optimal solution, because we could always improve any potential solution containing i by replacing i with the set J. Therefore we can disregard the i-th item altogether. In such cases, J is said to dominate i. (Note that this does not apply to bounded knapsack problems, since we may have already used up the items in J.) Finding dominance relations allows us to significantly reduce the size of the search space. There are several different types of dominance relations, &lt; refname = &quot; poirriez et all 2009 &quot; &gt; Vincent Poirriez, Nicola Yanev, Rumen Andonov (2009) A Hybrid Algorithm for the Unbounded Knapsack Problem, section 2) Discrete Optimization http://dx.doi.org/10.1016/j.disopt.2008.09.004 which all satisfy an inequality of the form: \qquad \sum_{j \in J} w_j\,x_j \ \le \alpha\,w_i, and \qquad \sum_{j \in J} v_j\,x_j \ \ge \alpha\,v_i\, for some x \in Z _+^n where \alpha\in Z_+ \,,J\subseteq N and i\not\in J. The vector x denotes the number of copies of each member of J. Collective dominance The i-th item is collectively dominated by J, written as i\ll J, if the total weight of some combination of items in J is less than w i and their total value is greater than v i. Formally, \qquad \sum_{j \in J} w_j\,x_j \ \le w_i and \qquad \sum_{j \in J} v_j\,x_j \ \ge v_i for some x \in Z _+^n, i.e. \alpha=1. Verifying this dominance is computationally hard, so it can only be used with a dynamic programming approach. In fact, this is equivalent to solving a smaller knapsack decision problem where2 V = v i, W = w i, and the items are restricted to J. Threshold dominance The i-th item is threshold dominated by J, written as i\prec\prec J, if some number of copies of i are dominated by J. Formally, \qquad \sum_{j \in J} w_j\,x_j \ \le \alpha\,w_i, and \qquad \sum_{j \in J} v_j\,x_j \ \ge \alpha\,v_i\, for some x \in Z _+^n and \alpha\geq 1. This is a generalization of collective dominance, first introduced in and used in the EDUK algorithm. The smallest such \alpha defines the threshold of the item i, written t_i =(\alpha-1)w_i. In this case, the optimal solution could contain at most \alpha-1 copies of i. Multiple dominance The i-th item is multiply dominated by a single item j, written as i\ll_{m} j, if i is dominated by some number of copies of j. Formally, \qquad w_j\,x_j \ \le w_i, and \qquad v_j\,x_j \ \ge v_i for some x_j \in Z _+ i.e. J=\{j\}, \alpha=1, x_j=\lfloor \frac{w_i}{w_j}\rfloor.This dominance could be efficiently used during preprocessing because it can be detected relatively easily. Modular dominance Let b be the best item, i.e. \frac{v_b}{w_b}\ge\frac{v_i}{w_i}\, for all i. This is the item with the greatest density of value. The i-th item is modularly dominated by a single item j, written as i\ll_\equiv j, if i is dominated by j plus several copies of b. Formally, w_j+tw_b \le w_i, and v_j +tv_b \ge v_i i.e. J=\{b,j\}, \alpha=1, x_b=t, x_j=1. [ Variations on the Knapsack Problem ] There are many variations of the knapsack problem that have arisen from the vast number of applications of the basic problem. The main variations occur by changing the number of some problem parameter such as the number of items, number of objectives, or even the number of knapsacks. [ Multi-objective Knapsack Problem ] This variation changes the goal of the individual filling the knapsack. Instead of one objective, such as maximizing the monetary profit, the objective could have several dimensions. For example there could be environmental or social concerns as well as economic goals. Problems frequently addressed include portfolio and transportation logistics optimizations { Chang, T. J., et al. Heuristics for Cardinality Constrained Portfolio Optimization.Technical Report, London SW7 2AZ, England: The Management School, ImperialCollege, May 1998 } { Chang, C. S., et al. &quot; Genetic Algorithm Based Bicriterion Optimization for TractionSubstations in DC Railway System. &quot; In Fogel [102], 11-16. } There are many heuristics for solving this variant, including the Ant Colony Optimization algorithm. { Fidanova S., Ant colony optimization and multiple knapsack problem, in: Renard, J.Ph. (Eds.), Handbook of Research onNature Inspired Computing for Economics ad Management, Idea Grup Inc., ISBN 1-59140-984-5, 2006, 498-509. } As a concrete example, suppose you ran a cruise ship. You have to decide how many famous comedians to hire. This boat can handle more than one ton of passengers and the entertainers must weigh less than 1000  lbs. Each comedian has a weight, brings in business based on their popularity and asks for a specific salary. In this example you have multiple objectives. You want, of course, to maximize the popularity of your entertainers while minimizing their salaries. Also, you want to have as many entertainers as possible. [ Multiple Knapsack Problem ] This variation is similar to the Bin Packing Problem. The concept is that there are multiple knapsacks. This may seem a trivial change, but it is not equivalent to adding to the capacity of the initial knapsack. This variation is used in many loading and scheduling problems in Operations Research and has an FPTAS { Chandra Chekuri and Sanjeev Khanna. 2000. A PTAS for the multiple knapsack problem. In Proceedings of the eleventh annual ACM-SIAM symposium on Discrete algorithms (SODA &apos;00). Society for Industrial and Applied Mathematics, Philadelphia, PA, USA, 213-222. } [ Quadratic Knapsack Problem ] The quadratic knapsack problem was first introduced by Gallo, Hammer, and Simeone in 1960. [ Quadratic knapsack problems Gallo, G.; Hammer, P. L.; Simeone, B. Mathematical Programming Studies 1980 12 132 –149 10.1007/BFb0120892 http://www.springerlink.com/content/x804231403086x51/ ] [ Subset-Sum Problem ] The subset sum problem, is a special case of the decision and 0-1 problems where each kind of item, the weight equals the value: w_i=v_i. In the field of cryptography, the term knapsack problem is often used to refer specifically to the subset sum problem and is commonly known as one of Karp&apos;s 21 NP-complete problems { Richard M. Karp (1972). &quot; Reducibility Among Combinatorial Problems &quot;. In R. E. Miller and J. W. Thatcher (editors). Complexity of Computer Computations. New York: Plenum. pp. 85–103 } [ See also ] List of knapsack problems Packing problem Cutting stock problem Continuous knapsack problem Combinatorial optimization Combinatorial auction [ Notes ] [ References ] [ Michael R. Garey Michael R. Garey David S. Johnson 1979 Computers and Intractability: A Guide to the Theory of NP-Completeness W.H. Freeman 0-7167-1045-5 ] [ Knapsack Problems Kellerer, Hans; Pferschy, Ulrich; Pisinger, David Springer 2004 3-540-40286-1 2161720 ] [ Knapsack problems: Algorithms and computer interpretations Martello Silvano Toth Paolo Wiley-Interscience 1990 0-471-92420-2 1086874 ] [ External links ] MIT Lecture on the Knapsack Problem and Dynamic Programming Lecture slides on the knapsack problem PYAsUKP: Yet Another solver for the Unbounded Knapsack Problem, with code taking advantage of the dominance relations in an hybrid algorithm, benchmarks and downloadable copies of some papers. Home page of David Pisinger with downloadable copies of some papers on the publication list (including &quot; Where are the hard knapsack problems? &quot; ) Knapsack Problem solutions in many languages at Rosetta Code Dynamic Programming algorithm to 0/1 Knapsack problem 0-1 Knapsack Problem in Python Interactive JavaScript branch-and-bound solver Solving 0-1-KNAPSACK with Genetic Algorithms in Ruby [ September 2010 ] Category:Cryptography Category:NP-complete problems Category:Operations research Category:Dynamic programming Category:Combinatorial optimization Category:Weakly NP-complete problems Category:Pseudo-polynomial time algorithms [ fr ] ca:Problema de la motxilla cs:Problém batohu de:Rucksackproblem es:Problema de la mochila fa:مسئله کوله‌پشتی fr:Problème du sac à dos ko:배낭 문제 it:Problema dello zaino nl:Knapzakprobleem ja:ナップサック問題 pl:Problem plecakowy pt:Problema da mochila ru:Задача о ранце sl:Preprosti problem nahrbtnika sv:Kappsäcksproblemet tr:Sırt çantası problemi uk:Задача пакування рюкзака vi:Bài toán xếp ba lô zh:背包问题