Synchronous dynamic random access memory (SDRAM) is dynamic random access memory (DRAM) that is synchronized with the system bus. Classic DRAM has an asynchronous interface, which means that it responds as quickly as possible to changes in control inputs. SDRAM has a synchronous interface, meaning that it waits for a clock signal before responding to control inputs and is therefore synchronized with the computer&apos;s system bus. The clock is used to drive an internal finite state machine that pipelines incoming commands. This allows the chip to have a more complex pattern of operation than an asynchronous DRAM, enabling higher speeds. Pipelining means that the chip can accept a new command before it has finished processing the previous one. In a pipelined write, the write command can be immediately followed by another command, without waiting for the data to be written to the memory array. In a pipelined read, the requested data appears after a fixed number of clock cycles after the read command (latency), clock cycles during which additional commands can be sent. (This delay is called the latency and is an important performance parameter to consider when purchasing SDRAM for a computer.) SDRAM is widely used in computer s; from the original SDRAM, further generations of DDR (or DDR1) and then DDR2 and DDR3 have entered the mass market, with DDR4 currently being designed and anticipated to be available in 2015. [ SDRAM history ] { PC100 } { DIMM package. } Although the concept of synchronous DRAM has been known since at least the 1970s and was used with early Intel processors, it was only in 1993 that SDRAM began its path to universal acceptance in the electronics industry. In 1993, Samsung introduced its KM48SL2000 synchronous DRAM, and by 2000, SDRAM had replaced virtually all other types of DRAM in modern computers, because of its greater performance. SDRAM latency is not inherently lower (faster) than asynchronous DRAM. Indeed, early SDRAM was somewhat slower than contemporaneous burst EDO DRAM due to the additional logic. The benefits of SDRAM&apos;s internal buffering come from its ability to interleave operations to multiple banks of memory, thereby increasing effective bandwidth. Today, virtually all SDRAM is manufactured in compliance with standards established by JEDEC, an electronics industry association that adopts open standards to facilitate interoperability of electronic components. JEDEC formally adopted its first SDRAM standard in 1993 and subsequently adopted other SDRAM standards, including those for DDR, DDR2 and DDR3 SDRAM. SDRAM is also available in registered varieties, for systems that require greater scalability such as server s and workstations. [ 2007 ], 168-pin SDRAM DIMM s are not used in new PC systems, and 184-pin DDR memory has been mostly superseded. DDR2 SDRAM is the most common type used with new PCs, and DDR3 motherboards and memory are widely available, and less expensive than still-popular DDR2 products. Today, the world&apos;s largest manufacturers of SDRAM include: Samsung Electronics, Panasonic, Micron Technology, and Hynix. [ SDRAM timing ] There are several limits on DRAM performance. Most noted is the read cycle time, the time between successive read operations to an open row. This time decreased from 10  ns for 100  MHz SDRAM to 5  ns for DDR-400, but has remained relatively unchanged through DDR2-800 and DDR3-1600 generations. However, by operating the interface circuitry at increasingly higher multiples of the fundamental read rate, the achievable bandwidth has increased rapidly. Another limit is the CAS latency, the time between supplying a column address and receiving the corresponding data. Again, this has remained relatively constant at 10–15 ns through the last few generations of DDR SDRAM. In operation, CAS latency is a specific number of clock cycles programmed into the SDRAM&apos;s mode register and expected by the DRAM controller. Any value may be programmed, but the SDRAM will not operate correctly if it is too low. At higher clock rates, the useful CAS latency in clock cycles naturally increases. 10–15  ns is 2–3 cycles (CL2–3) of the 200  MHz clock of DDR-400 SDRAM, CL4-6 for DDR2-800, and CL8-12 for DDR3-1600. Slower clock cycles will naturally allow lower numbers of CAS latency cycles. SDRAM modules have their own timing specifications, which may be slower than those of the chips on the module. When 100  MHz SDRAM chips first appeared, some manufacturers sold &quot; 100  MHz &quot; modules that could not reliably operate at that clock rate. In response, Intel published the PC100 standard, which outlines requirements and guidelines for producing a memory module that can operate reliably at 100  MHz. This standard was widely influential, and the term &quot; PC100 &quot; quickly became a common identifier for 100  MHz SDRAM modules, and modules are now commonly designated with &quot; PC &quot; -prefixed numbers ( PC66, PC100 or PC133 - although the actual meaning of the numbers has changed). [ SDR SDRAM ] { Sound Blaster X-Fi Fatal1ty Pro uses two } { Micron 48LC32M8A2-75 C SDRAM chips working at 133  MHz (7.5  ns) 8-bit wide } [ SDRAM Part Catalog http://www.micron.com/products/dram/sdram/partlist ] { 070928 micron.com } Originally simply known as SDRAM, single data rate SDRAM can accept one command and transfer one word of data per clock cycle. Typical clock frequencies are 100 and 133  MHz. Chips are made with a variety of data bus sizes (most commonly 4, 8 or 16 bits), but chips are generally assembled into 168-pin DIMM s that read or write 64 (non-ECC) or 72 ( ECC ) bits at a time. Use of the data bus is intricate and thus requires a complex DRAM controller circuit. This is because data written to the DRAM must be presented in the same cycle as the write command, but reads produce output 2 or 3 cycles after the read command. The DRAM controller must ensure that the data bus is never required for a read and a write at the same time. Typical SDR SDRAM clock rates are 66, 100, and 133  MHz (periods of 15, 10, and 7.5  ns). Clock rates up to 150  MHz were available for performance enthusiasts. [ SDRAM control signals ] All commands are timed relative to the rising edge of a clock signal. In addition to the clock, there are 6 control signals, mostly active low, which are sampled on the rising edge of the clock: CKE Clock Enable. When this signal is low, the chip behaves as if the clock has stopped. No commands are interpreted and command latency times do not elapse. The state of other control lines is not relevant. The effect of this signal is actually delayed by one clock cycle. That is, the current clock cycle proceeds as usual, but the following clock cycle is ignored, except for testing the CKE input again. Normal operations resume on the rising edge of the clock after the one where CKE is sampled high. Put another way, all other chip operations are timed relative to the rising edge of a masked clock. The masked clock is the logical AND of the input clock and the state of the CKE signal during the previous rising edge of the input clock. /CS Chip Select. When this signal is high, the chip ignores all other inputs (except for CKE), and acts as if a NOP command is received. DQM Data Mask. (The letter Q appears because, following digital logic conventions, the data lines are known as &quot; DQ &quot; lines.) When high, these signals suppress data I/O. When accompanying write data, the data is not actually written to the DRAM. When asserted high two cycles before a read cycle, the read data is not output from the chip. There is one DQM line per 8 bits on a x16 memory chip or DIMM. /RAS Row Address Strobe. Despite the name, this is not a strobe, but rather simply a command bit. Along with /CAS and /WE, this selects one of 8 commands. /CAS Column Address Strobe. Despite the name, this is not a strobe, but rather simply a command bit. Along with /RAS and /WE, this selects one of 8 commands. /WE Write enable. Along with /RAS and /CAS, this selects one of 8 commands. This generally distinguishes read-like commands from write-like commands. SDRAM devices are internally divided into 2 or 4 independent internal data banks. One or two bank address inputs (BA0 and BA1) select which bank a command is directed toward. Many commands also use an address presented on the address input pins. Some commands, which either do not use an address, or present a column address, also use A10 to select variants. The commands understood are as follows: /CS /RAS /CAS /WE BAn A10 An Command H x x x x x x Command inhibit (No operation) L H H H x x x No operation L H H L x x x Burst Terminate: stop a burst read or burst write in progress. L H L H bank L column Read: Read a burst of data from the currently active row. L H L H bank H column Read with auto precharge: As above, and precharge (close row) when done. L H L L bank L column Write: Write a burst of data to the currently active row. L H L L bank H column Write with auto precharge: As above, and precharge (close row) when done. L L H H bank row Active (activate): open a row for Read and Write commands. L L H L bank L x Precharge: Deactivate current row of selected bank. L L H L x H x Precharge all: Deactivate current row of all banks. L L L H x x x Auto refresh: Refresh one row of each bank, using an internal counter. All banks must be precharged. L L L L 0 0 mode Load mode register: A0 through A9 are loaded to configure the DRAM chip. The most significant settings are CAS latency (2 or 3 cycles) and burst length (1, 2, 4 or 8 cycles) { The various DDRx SDRAM standards use essentially the same commands, with minor additions. Additional mode registers are distinguished using the bank address bits, and a third bank address bit is added. } [ SDRAM operation ] A 512   MB SDRAM DIMM (which contains 512 MiB = [ 512 &amp; times; 2 20 bytes ] = 536,870,912 bytes exactly) might be made of 8 or 9 SDRAM chips, each containing 512   Mbit of storage, and each one contributing 8 bits to the DIMM&apos;s 64- or 72-bit width. A typical 512  Mbit SDRAM chip internally contains 4 independent 16  Mbyte memory banks. Each bank is an array of 8,192 rows of 16,384 bits each. A bank is either idle, active, or changing from one to the other. The Active command activates an idle bank. It presents a 2-bit bank address (BA0 BA1) and a 13-bit row address (A0 A12), and causes a read of that row into the bank&apos;s array of all 16,384 column sense amplifiers. This is also known as &quot; opening &quot; the row. This operation has the side effect of refresh ing the dynamic (capacitive) memory storage cells of that row. Once the row has been activated or &quot; opened &quot;, Read and Write commands are possible to that row. Activation requires a minimum amount of time, called the row-to-column delay, or t RCD before reads or writes to it may occur. This time, rounded up to the next multiple of the clock period, specifies the minimum number of wait cycles between an Active command, and a Read or Write command. During these wait cycles, additional commands may be sent to other banks; because each bank operates completely independently. Both Read and Write commands require a column address. Because each chip accesses 8 bits of data at a time, there are 2048 possible column addresses thus requiring only 11 address lines (A0 A9, A11). When a Read command is issued, the SDRAM will produce the corresponding output data on the DQ lines in time for the rising edge of the clock 2 or 3 clock cycles later (depending on the configured CAS latency). Subsequent words of the burst will be produced in time for subsequent rising clock edges. A Write command is accompanied by the data to be written driven on to the DQ lines during the same rising clock edge. It is the duty of the memory controller to ensure that the SDRAM is not driving read data on to the DQ lines at the same time that it needs to drive write data on to those lines. This can be done by waiting until a read burst has finished, by terminating a read burst, or by using the DQM control line. When the memory controller needs to access a different row, it must first return that bank&apos;s sense amplifiers to an idle state, ready to sense the next row. This is known as a &quot; precharge &quot; operation, or &quot; closing &quot; the row. A precharge may be commanded explicitly, or it may be performed automatically at the conclusion of a read or write operation. Again, there is a minimum time, the row precharge delay, t RP, which must elapse before that bank is fully idle and it may receive another activate command. Although refreshing a row is an automatic side effect of activating it, there is a minimum time for this to happen, which requires a minimum row access time t RAS delay between an Active command opening a row, and the corresponding precharge command closing it. This limit is usually dwarfed by desired read and write commands to the row, so its value has little effect on typical performance. [ Command interactions ] The no operation command is always permitted. The load mode register command requires that all banks be idle, and a delay afterward for the changes to take effect. The auto refresh command also requires that all banks be idle, and takes a refresh cycle time t RFC to return the chip to the idle state. (This time is usually equal to t RCD +t RP.) The only other command that is permitted on an idle bank is the active command. This takes, as mentioned above, t RCD before the row is fully open and can accept read and write commands. When a bank is open, there are four commands permitted: read, write, burst terminate, and precharge. Read and write commands begin bursts, which can be interrupted by following commands. [ Interrupting a read burst ] A read, burst terminate, or precharge command may be issued at any time after a read command, and will interrupt the read burst after the configured CAS latency. So if a read command is issued on cycle 0, another read command is issued on cycle 2, and the CAS latency is 3, then the first read command will begin bursting data out during cycles 3 and 4, then the results from the second read command will appear beginning with cycle 5. If the command issued on cycle 2 were burst terminate, or a precharge of the active bank, then no output would be generated during cycle 5. Although the interrupting read may be to any active bank, a precharge command will only interrupt the read burst if it is to the same bank or all banks; a precharge command to a different bank will not interrupt a read burst. To interrupt a read burst by a write command is possible, but more difficult. It can be done, if the DQM signal is used to suppress output from the SDRAM so that the memory controller may drive data over the DQ lines to the SDRAM in time for the write operation. Because the effects of DQM on read data are delayed by 2 cycles, but the effects of DQM on write data are immediate, DQM must be raised (to mask the read data) beginning at least two cycles before write command, but must be lowered for the cycle of the write command (assuming you want the write command to have an effect). Doing this in only two clock cycles requires careful coordination between the time the SDRAM takes to turn off its output on a clock edge and the time the data must be supplied as input to the SDRAM for the write on the following clock edge. If the clock frequency is too high to allow sufficient time, three cycles may be required. If the read command includes auto-precharge, the precharge begins the same cycle as the interrupting command. [ SDRAM burst ordering ] A modern microprocessor with a cache will generally access memory in units of cache line s. To transfer a 64-byte cache line requires 8 consecutive accesses to a 64-bit DIMM, which can all be triggered by a single read or write command by configuring the SDRAM chips, using the mode register, to perform 8-word bursts. A cache line fetch is typically triggered by a read from a particular address, and SDRAM allows the &quot; critical word &quot; of the cache line to be transferred first. ( &quot; Word &quot; here refers to the width of the SDRAM chip or DIMM, which is 64 bits for a typical DIMM.) SDRAM chips support two possible conventions for the ordering of the remaining words in the cache line. Bursts always access an aligned block of BL consecutive words beginning on a multiple of BL. So, for example, a 4-word burst access to any column address from 4 to 7 will return words 4 7. The ordering, however, depends on the requested address, and the configured burst type option: sequential or interleaved. Typically, a memory controller will require one or the other. When the burst length is 1 or 2, the burst type does not matter. For a burst length of 1, the requested word is the only word accessed. For a burst length of 2, the requested word is accessed first, and the other word in the aligned block is accessed second. This is the following word if an even address was specified, and the previous word if an odd address was specified. For the sequential burst mode, later words are accessed in increasing address order, wrapping back to the start of the block when the end is reached. So, for example, for a burst length of 4, and a requested column address of 5, the words would be accessed in the order 5-6-7-4. If the burst length were 8, the access order would be 5-6-7-0-1-2-3-4. This is done by adding a counter to the column address, and ignoring carries past the burst length. The interleaved burst mode computes the address using an exclusive or operation between the counter and the address. Using the same starting address of 5, a 4-word burst would return words in the order 5-4-7-6. An 8-word burst would be 5-4-7-6-1-0-3-2. Although more confusing to humans, this can be easier to implement in hardware, and is preferred by Intel microprocessors. If the requested column address is at the start of a block, both burst modes return data in the same sequential sequence 0-1-2-3-4-5-6-7. The difference only matters if fetching a cache line from memory in critical-word-first order. [ SDRAM mode register ] Single data rate SDRAM has a single 10-bit programmable mode register. Later double-data-rate SDRAM standards add additional mode registers, addressed using the bank address pins. For SDR SDRAM, the bank address pins and address lines A10 and above are ignored, but should be zero during a mode register write. The bits are M9 through M0, presented on address lines A9 through A0 during a load mode register cycle. M9: Write burst mode. If 0, writes use the read burst length and mode. If 1, all writes are non-burst (single location). M8, M7: Operating mode. Reserved, and must be 00. M6, M5, M4: CAS latency. Generally only 010 (CL2) and 011 (CL3) are legal. Specifies the number of cycles between a read command and data output from the chip. The chip has a fundamental limit on this value in nanoseconds; during initialization, the memory controller must use its knowledge of the clock frequency to translate that limit into cycles. M3: Burst type. 0 - requests sequential burst ordering, while 1 requests interleaved burst ordering. M2, M1, M0: Burst length. Values of 000, 001, 010 and 011 specify a burst size of 1, 2, 4 or 8 words, respectively. Each read (and write, if M9 is 0) will perform that many accesses, unless interrupted by a burst stop or other command. A value of 111 specifies a full-row burst. The burst will continue until interrupted. Full-row bursts are only permitted with the sequential burst type. Later (double data rate) SDRAM standards use more mode register bits, and provide additional extended mode registers. The register number is encoded on the bank address pins during the load mode register cycle. For example, DDR2 SDRAM has a 13-bit mode register, a 13-bit EMR1, and uses 5 bits in EMR2. [ Auto refresh ] It is possible to refresh a RAM chip by opening and closing (activating and precharging) each row in each bank. However, to simplify the memory controller, SDRAM chips support an &quot; auto refresh &quot; command, which performs these operations to one row in each bank simultaneously. The SDRAM also maintains an internal counter, which iterates over all possible rows. The memory controller must simply issue a sufficient number of auto refresh commands (one per row, 4096 in the example we have been using) every refresh interval (t REF = 64 ms is a common value). All banks must be idle (closed, precharged) when this command is issued. [ Low power modes ] As mentioned, the clock enable (CKE) input can be used to effectively stop the clock to an SDRAM. The CKE input is sampled each rising edge of the clock, and if it is low, the following rising edge of the clock is ignored for all purposes other than checking CKE. As long as CKE is low, it is permissible to change the clock rate, or even stop the clock entirely. If CKE is lowered while the SDRAM is performing operations, it simply &quot; freezes &quot; in place until CKE is raised again. If the SDRAM is idle (all banks precharged, no commands in progress) when CKE is lowered, the SDRAM automatically enters power-down mode, consuming minimal power until CKE is raised again. This must not last longer than the maximum refresh interval t REF, or memory contents may be lost. It is legal to stop the clock entirely during this time for additional power savings. Finally, if CKE is lowered at the same time as an auto-refresh command is sent to the SDRAM, the SDRAM enters self-refresh mode. This is like power down, but the SDRAM uses an on-chip timer to generate internal refresh cycles as necessary. The clock may be stopped during this time. While self-refresh mode consumes slightly more power than power-down mode, it allows the memory controller to be disabled entirely, which commonly more than makes up the difference. SDRAM designed for battery-powered devices offers some additional power-saving options. One is temperature-dependent refresh; an on-chip temperature sensor reduces the refresh rate at lower temperatures, rather than always running it at the worst-case rate. Another is selective refresh, which limits self-refresh to a portion of the DRAM array. The fraction which is refreshed is configured using an extended mode register. The third, implemented in Mobile DDR (LPDDR) and LPDDR2 is &quot; deep power down &quot; mode, which invalidates the memory and requires a full reinitialization to exit from. This is activated by sending a &quot; burst terminate &quot; command while lowering CKE. [ Generations of SDRAM ] [ SDR SDRAM (Single Data Rate synchronous DRAM) ] This type of SDRAM is slower than the DDR variants, because only one word of data is transmitted per clock cycle (single data rate).But this type is also faster than its predecessors EDO-RAM and FPM-RAM which took typically 2 or 3 clocks to transfer one word of data. [ DDR SDRAM (DDR1) ] [ DDR SDRAM ] While the access latency of DRAM is fundamentally limited by the DRAM array, DRAM has very high potential bandwidth because each internal read is actually a row of many thousands of bits. To make more of this bandwidth available to users, a double data rate interface was developed. This uses the same commands, accepted once per cycle, but reads or writes two words of data per clock cycle. The DDR interface accomplishes this by reading and writing data on both the rising and falling edges of the clock signal. In addition, some minor changes to the SDR interface timing were made in hindsight, and the supply voltage was reduced from 3.3 to 2.5  V. As a result, DDR SDRAM is not backwards compatible with SDR SDRAM. DDR SDRAM (sometimes called DDR1 for greater clarity) doubles the minimum read or write unit; every access refers to at least two consecutive words. Typical DDR SDRAM clock rates are 133, 166 and 200  MHz (7.5, 6, and 5 ns/cycle), generally described as DDR-266, DDR-333 and DDR-400 (3.75, 3, and 2.5  ns per beat). Corresponding 184-pin DIMMs are known as PC-2100, PC-2700 and PC-3200. Performance up to DDR-550 (PC-4400) is available for a price. [ DDR2 SDRAM ] [ DDR2 SDRAM ] DDR2 SDRAM is very similar to DDR SDRAM, but doubles the minimum read or write unit again, to 4 consecutive words. The bus protocol was also simplified to allow higher performance operation. (In particular, the &quot; burst terminate &quot; command is deleted.) This allows the bus rate of the SDRAM to be doubled without increasing the clock rate of internal RAM operations; instead, internal operations are performed in units 4 times as wide as SDRAM. Also, an extra bank address pin (BA2) was added to allow 8 banks on large RAM chips. Typical DDR2 SDRAM clock rates are 200, 266, 333 or 400  MHz (periods of 5, 3.75, 3 and 2.5  ns), generally described as DDR2-400, DDR2-533, DDR2-667 and DDR2-800 (periods of 2.5, 1.875, 1.5 and 1.25  ns). Corresponding 240-pin DIMMS are known as PC2-3200 through PC2-6400. DDR2 SDRAM is now available at a clock rate of 533  MHz generally described as DDR2-1066 and the corresponding DIMMs are known as PC2-8500 (also named PC2-8600 depending on the manufacturer). Performance up to DDR2-1250 (PC2-10000) is available for a price. Note that because internal operations are at 1/2 the clock rate, DDR2-400 memory (internal clock rate 100  MHz) has somewhat higher latency than DDR-400 (internal clock rate 200  MHz). [ DDR3 SDRAM ] [ DDR3 SDRAM ] DDR3 continues the trend, doubling the minimum read or write unit to 8 consecutive words. This allows another doubling of bandwidth and external bus rate without having to change the clock rate of internal operations, just the width. To maintain 800–1600  M transfers/s (both edges of a 400–800  MHz clock), the internal RAM array has to perform 100–200  M fetches per second. Again, with every doubling, the downside is the increased latency. As with all DDR SDRAM generations, commands are still restricted to one clock edge and command latencies are given in terms of clock cycles, which are half the speed of the usually quoted transfer rate (a CAS latency of 8 with DDR3-800 is 8/(400  MHz) = 20 ns, exactly the same latency of CAS2 on PC100 SDR SDRAM). DDR3 memory chips are being made commercially, [ http://www.simmtester.com/page/news/showpubnews.asp?num%3D145 What is DDR memory? ] and computer systems using them were available from the second half of 2007, [ http://www.tomshardware.com/2007/06/05/pipe_dreams_six_p35-ddr3_motherboards_compared/ Pipe Dreams: Six P35-DDR3 Motherboards Compared June 5, 2007 Thomas Soderstrom Tom&apos;s Hardware ] with significant usage from 2008 onwards. [ http://news.softpedia.com/news/AMD-to-Adopt-DDR3-in-Three-Years-13486.shtml AMD to Adopt DDR3 in Three Years ] Initial clock rates were 400 and 533  MHz, which are described as DDR3-800 and DDR3-1066 (PC3-6400 and PC3-8500 modules), but 667 and 800  MHz, described as DDR3-1333 and DDR3-1600 (PC3-10600 and PC3-12800 modules) are now common. [ http://www.anandtech.com/printarticle.aspx?i%3D3045 Super Talent &amp; TEAM: DDR3-1600 Is Here! July 20, 2007 Wesly Fink Anandtech ] Performance up to DDR3-2200 (PC3 17600 modules) are available for a price. [ http://www.brightsideofnews.com/news/2009/7/28/a-data-launches-ddr3-2200-with-2oz-pcb.aspx A-Data launches DDR3-2200 with 2oz. copper PCB 28 July 2009 Thomas Jørgen Jacobsen ] [ DDR4 SDRAM ] [ DDR4 SDRAM ] DDR4 SDRAM will be the successor to DDR3 SDRAM. It was revealed at the Intel Developer Forum in San Francisco in 2008, and is due to be released to market during 2011. The timing has varied considerably during its development - it was originally expected to be released in 2012, { DDR4 PDF page 23 } and later (during 2010) expected to be released in 2015, { http://www.semiaccurate.com/2010/08/16/ddr4-not-expected-until-2015/ } before samples were announced in early 2011 and manufacturers began to announce that commercial production and release to market was anticipated in 2012. DDR4 is expected to reach mass market adoption around 2015, which is comparable with the approximately 5 years taken for DDR3 to achieve mass market transition over DDR2. The new chips are expected to run at 1.2   V or less, { Looking forward to DDR4 } { DDR3 successor } versus the 1.5  V of DDR3 chips, and have in excess of 2 billion data transfer s per second. They are expected to be introduced at frequency rates of 2133  MHz, estimated to rise to a potential 4266  MHz [ http://www.xbitlabs.com/news/memory/display/20100816124343_Next_Generation_DDR4_Memory_to_Reach_4_266GHz_Report.html Next-Generation DDR4 Memory to Reach 4.266GHz - Report August 16, 2010 Xbitlabs.com 2011-01-03 ] and lowered voltage of 1.05  V [ http://www.hardware-infos.com/news.php?news%3D2332 IDF: DDR4 memory targeted for 2012 hardware-infos.com German 2009-06-16 ] { English translation } by 2013. DDR4 will not double the internal prefetch width again, but will use the same 8n prefetch as DDR3. [ http://www.jedec.org/news/pressreleases/jedec-announces-key-attributes-upcoming-ddr4-standard JEDEC Announces Key Attributes of Upcoming DDR4 Standard JEDEC 2011-08-22 2011-01-06 ] Thus, it will be necessary to interleave reads from several banks to keep the data bus busy. In February 2009, Samsung validated 40  nm DRAM chips, considered a &quot; significant step &quot; towards DDR4 development [ http://www.tgdaily.com/content/view/41316/139/ Samsung hints to DDR4 with first validated 40  nm DRAM Gruener Wolfgang February 4, 2009 tgdaily.com 2009-06-16 ] since as of 2009, current DRAM chips were only beginning to migrate to a 50  nm process. [ http://www.dailytech.com/DDR3%2BWill%2Bbe%2BCheaper%2BFaster%2Bin%2B2009/article13977.htm DDR3 Will be Cheaper, Faster in 2009 Jansen Ng January 20, 2009 dailytech.com 2009-06-17 ] In January 2011, Samsung announced the completion and release for testing of a 30  nm 2  GB DDR4 DRAM module. It has a maximum bandwidth of 2.13  Gbit/s at 1.2  V, uses pseudo open drain technology and draws 40% less power than an equivalent DDR3 module. [ Samsung Develops Industry&apos;s First DDR4 DRAM, Using 30nm Class Technology http://www.samsung.com/us/business/semiconductor/newsView.do?news_id%3D1202 Samsung 2011-03-13 2011-01-04 ] { http://www.techspot.com/news/41818-samsung-develops-ddr4-memory-up-to-40-more-efficient.html } [ Feature map ] [ V cc &amp; #61; 3.3 V ] [ V cc &amp; #61; 2.5 V ] [ 2.5 - 7.5 ns ] [ EDA DesignLine, januari 12, 2007, The outlook for DRAMs in consumer electronics http://www.edadesignline.com/196900432?printableArticle%3Dtrue ] [ 1.25 - 5 ns ] [ V cc ≤ 1.2 V ] Type Feature changes SDRAM Signal: LVTTL DDR1 Access is ≥2 words Double clocked per cycle Signal: SSTL_2 (2.5V) 100622 edadesignline.com DDR2 Access is ≥4 words &quot; Burst terminate &quot; removed 4 units used in parallel per cycle Internal operations are at 1/2 the clock rate. Signal: SSTL_18 (1.8V) DDR3 Access is ≥8 words Signal: SSTL_15 (1.5V) Much longer CAS latencies DDR4 point-to-point (single module per channel) [ Failed successors ] In addition to DDR, there were several other proposed memory technologies to succeed SDR SDRAM. [ Rambus DRAM (RDRAM) ] RDRAM was a proprietary technology that competed against DDR. Its relatively high price and disappointing performance (resulting from high latencies and a narrow 16-bit data channel versus DDR&apos;s 64 bit channel) caused it to lose the race to succeed SDR DRAM. [ Synchronous-Link DRAM (SLDRAM) ] SLDRAM boasted higher performance and competed against RDRAM. It was developed during the late 1990s by the SLDRAM Consortium, which consisted of about 20 major computer industry manufacturers. It is an open standard and does not require licensing fees. The specifications called for a 64-bit bus running at a 200, 300 or 400  MHz clock frequency. This is achieved by all signals being on the same line and thereby avoiding the synchronization time of multiple lines. Like DDR SDRAM, SLDRAM uses a double-pumped bus, giving it an effective speed of 400, [ http://www.tomshardware.com/reviews/ram-guide%2C89-15.html RAM Guide: SLDRAM Dean Kent Tom&apos;s Hardware 1998-10-24 2011-01-01 ] 600, [ http://icwic.cn/icwic/data/pdf/cd/cd011/12452.pdf HYSL8M18D600A 600 Mb/s/pin 8M x 18 SLDRAM data sheet Hyundai Electronics 1997-12-20 2011-12-27 ] or 800  MT/s. SLDRAM used an 11-bit command bus (10 command bits CA9:0 plus one start-of-command FLAG line) to transmit 40-bit command packets on 4 consecutive edges of a differential command clock (CCLK/CCLK#). Unlike SDRAM, there were no per-chip select signals; each chip was assigned an ID when reset, and the command contained the ID of the chip that should process it. Data was transferred in 4- or 8-word bursts across an 18-bit (per chip) data bus, using one of two differential data clocks (DCLK0/DCLK0# and DCLK1/DCLK1#). Unlike standard SDRAM, the clock was generated by the data source (the SLDRAM chip in the case of a read operation) and transmitted in the same direction as the data, greatly reducing data skew. To avoid the need for a pause when the source of the DCLK changes, each command specified which DCLK pair it would use. [ http://icwic.cn/icwic/data/pdf/cd/cd011/12407.pdf 32–33 SLD4M18DR400 400 Mb/s/pin 4M x 18 SLDRAM data sheet SLDRAM Inc. 1998-07-09 2011-12-27 ] The basic read/write command consisted of (beginning with CA9 of the first word): SLDRAM Read, write or row op request packet FLAG CA9 CA8 CA7 CA6 CA5 CA4 CA3 CA2 CA1 CA0 1 ID8 Device ID ID0 CMD5 0 Command code CMD0 Bank Row 0 Row (continued) 0 0 0 0 0 Column 9 bits of device ID 6 bits of command 3 bits of bank address 10 or 11 bits of row address 5 or 4 bits spare for row or column expansion 7 bits of column address Individual devices had 8-bit IDs. The 9th bit of the ID sent in commands was used to address multiple devices. Any aligned power-of-2 sized group could be addressed. If the transmitted msbit was set, all least-significant bits up to and including the least-significant 0 bit of the transmitted address were ignored for &quot; is this addressed to me? &quot; purposes. (If the ID8 bit is actually considered less significant than ID0, the unicast address matching becomes a special case of this pattern.) A read/write command had the msbit clear: CMD5=0 CMD4=1 to open (activate) the specified row; CMD4=0 to use the currently open row CMD3=1 to transfer an 8-word burst; CMD3=0 for a 4-word burst CMD2=1 for a write, CMD2=0 for a read CMD1=1 to close the row after this access; CMD1=0 to leave it open CMD0 selects the DCLK pair to use (DCLK1 or DCLK0) A notable omission from the specification was per-byte write enables; it was designed for systems with cache s and ECC memory, which always write in multiples of a cache line. Additional commands (with CMD5 set) opened and closed rows without a data transfer, performed refresh operations, read or wrote configuration registers, and performed other maintenance operations. Most of these commands supported an additional 4-bit sub-ID (sent as 5 bits, using the same multiple-destination encoding as the primary ID) which could be used to distinguish devices that were assigned the same primary ID because they were connected in parallel and always read/written at the same time. There were a number of 8-bit control registers and 32-bit status registers to control various device timing parameters. [ Virtual Channel Memory (VCM) SDRAM ] VCM was a proprietary type of SDRAM that was designed by NEC, but released as an open standard with no licensing fees. VCM creates a state in which the various system processes can be assigned their own virtual channel, thus increasing the overall system efficiency by avoiding the need to have processes share buffer space. This is accomplished by creating different &quot; blocks &quot; of memory, allowing each individual memory block to interface separately with the memory controller and have its own buffer space. VCM has higher performance than SDRAM because it has significantly lower latencies. The technology was a potential competitor of RDRAM because VCM was not nearly as expensive as RDRAM was. A Virtual Channel Memory (VCM) module is mechanically and electrically compatible with standard SDRAM, but must be recognized by the memory controller. Few motherboards were ever produced with VCM support. [ See also ] GDDR (graphics DDR) and its subtypes GDDR2, GDDR3, GDDR4, and GDDR5 SDRAM latency List of device bandwidths Serial presence detect - EEPROM with timing data on SDRAM modules SDRAM Tutorial - Flash website built by Tel-Aviv University students A concise but thorough review of SDRAM architecture/terminology and command timing dependencies in High-Performance DRAM System Design Constraints and Considerations, a master thesis from the University of Maryland. [ References ] ar:إس دي رام ca:SDRAM cs:SDR SDRAM de:Synchronous Dynamic Random Access Memory et:SDRAM es:SDRAM eu:SDRAM fr:SDRAM ko:SDR SDRAM id:SDRAM it:SDRAM he:SDRAM ja:SDRAM nl:SDRAM pl:SDRAM pt:SDR SDRAM ru:SDRAM sk:SDRAM fi:DRAM#SDRAM sv:SDRAM tr:SDRAM uk:SDRAM zh:SDRAM